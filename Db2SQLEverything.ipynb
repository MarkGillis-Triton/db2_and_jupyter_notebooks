{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do Almost Everything (to Administer Db2) via SQL and Jupyter Notebook\n",
    "This Notebook is designed to share the SQL and python code related to the presentation titled How to do Almost Everything (to Administer Db2) via SQL and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "Run at your own risk. Understand what each cell is doing before executing it. This is not a perfect document or perfect code. Depending on your system, some parts may or may not work. There are no guarantees or support. Interpretation of a skilled Db2 DBA is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "This notebook is covered under a GNU General Public License. Details are available at https://github.com/ecrooks/db2_and_jupyter_notebooks/blob/master/LICENSE.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for running\n",
    "1. Install Jupyter Notebook using Anaconda (https://www.anaconda.com/distribution/)\n",
    "    - Anaconda can be installed on your laptop or a vm on your laptop - anywhere you can connect to the databases in question. This works well if you are working with it alone, or have to connect to different vpns to connect to different databases\n",
    "    - Anaconda can be installed on a central VM or server that can connect to the databases you wish to work with. This works well if you are working with a team of DBAs and only need to work with databases on that one network. Anaconda works just fine on Ubuntu if you are looking for a free option\n",
    "    - Anaconda can be installed directly on the database server. This is generally my last choice, as I would rather not run an http server on my database server. I also rarely only care about one database server.\n",
    "1. Copy this notebook to the computer you've installed Jupyter Notebook on. I'll refer to this as your Jupyter Notebook server. \n",
    "1. Create a separate file to store enviornment variables. I've called mine ember_variables.py, and I run it in a cell below. This allows you to easily share the notebook without also sharing your ids and passwords and other sensative information. This also makes using git or other source control easy so you can keep notebooks updated across multiple locations. The format for this file is laid out below.\n",
    "\n",
    "### Format for variables file\n",
    "The ember_variables.py file has a format like this:\n",
    "```python\n",
    "NA1_User='yourid'\n",
    "NA1_PW='yourpw'\n",
    "\n",
    "NA1_Host='server1.example.com'\n",
    "NA1_insts = ('db2inst1', 'db2inst2', 'db2inst3', 'db2inst4')\n",
    "NA1_ports = {'db2inst1': 50001, 'db2inst2': 50002, 'db2inst3':50003, 'db2inst4':50004}\n",
    "NA1_dbs = {'db2inst1': ['SAMPLE1'], 'db2inst2': ['SAMPLE2'], 'db2inst3':['SAMPLE3'], 'db2inst4':['SAMPLE4','SAMPLE5']}\n",
    "```\n",
    "Feel free to structure things differently and if you have any good ideas in this area, please share them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the enviornment\n",
    "### Install Libraries\n",
    "Run the following cell if it is the first time using this notebook on a specific jupyter notebook server. If anything is installed, restart the kernel using the 'Kernel' menu at the top of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "os.environ['IBM_DB_HOME']='C:\\Program Files\\IBM\\SQLLIB'\n",
    "\n",
    "# Check to see if the libraries already have been installed\n",
    "import importlib\n",
    "\n",
    "# Check for ibm_db_sa.  If it exists, it's safe to assume that the other requirements\n",
    "# are already installed.\n",
    "spec = importlib.util.find_spec(\"ibm_db_sa\")\n",
    "if spec is None:\n",
    "    print(\"Installing prerequisites.\")\n",
    "    !pip install ipython-sql\n",
    "    !pip install \"ibm-db==2.0.8a\"\n",
    "    !pip install ibm_db_sa\n",
    "else:\n",
    "    print(\"sql magic, ibm_db and ibm_db_sa already installed.\")\n",
    "spec = importlib.util.find_spec(\"jupyter_contrib_nbextensions\")\n",
    "if spec is None:\n",
    "    print(\"Installing prerequisites.\")\n",
    "    !pip install jupyter_contrib_nbextensions\n",
    "    !pip install jupyter_nbextensions_configurator\n",
    "else:\n",
    "    print(\"jupyter_contrib_nbextensions is already installed.\")\n",
    "spec = importlib.util.find_spec(\"sqlparse\")\n",
    "if spec is None:\n",
    "    print(\"Installing prerequisites.\")\n",
    "    !pip install sqlparse\n",
    "else:\n",
    "    print(\"sqlparse already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the Kernel if this is your first time installing any of the above. The next steps will fail unless you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the modules and load the SQL magic\n",
    "Required each time the kernel for this notebook is started or restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_db\n",
    "import ibm_db_sa\n",
    "import sqlalchemy\n",
    "%load_ext sql\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import nbextensions\n",
    "import sqlparse\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Basic Variables and Connect to Database\n",
    "Connect to the database. Change the values in your variables file to match the environment you're connecting to. The format for this file is provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filename for passwords\n",
    "filename = 'ember_variables.py'\n",
    "# source the file\n",
    "%run $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the database connection Cell\n",
    "# It will prompt the user for a PW. PWs should not be stored.\n",
    "user=local_User\n",
    "host=local_Host\n",
    "inst='db2inst1'\n",
    "\n",
    "db=local_dbs[inst][0]\n",
    "port=local_ports[inst]\n",
    "\n",
    "password = getpass.getpass('Enter password for '+user)\n",
    "\n",
    "%sql db2+ibm_db://$user:$password@$host:$port/$db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure SQL Magic in a few nice ways\n",
    "%config SqlMagic.style = 'MSWORD_FRIENDLY'\n",
    "pd.set_option('max_rows', 4096)\n",
    "pd.set_option('max_columns', 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that may be used in other cells\n",
    "def highlight_equals(s,threshold,column):\n",
    "    is_max = pd.Series(data=False, index=s.index)\n",
    "    is_max[column] = s.loc[column] == threshold\n",
    "    print(type(is_max))\n",
    "    return ['background-color: yellow' if is_max.any() else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Database-Level Authorities and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Users with an Authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql connectauth << select grantee \n",
    "from syscat.dbauth \n",
    "where connectauth='Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(connectauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Authorities for a User the Old-Fashioned Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql id_auth << select dbadmauth\n",
    "\t, connectauth\n",
    "\t, dataaccessauth \n",
    "from syscat.dbauth \n",
    "where grantee='USER1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(id_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Table Permissions for a User the Old-Fashioned Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql obj_perms_by_id << select substr(tabschema,1,8) as tabschema\n",
    "\t, substr(tabname,1,18) as tabname\n",
    "\t, controlauth\n",
    "\t, deleteauth\n",
    "\t, insertauth\n",
    "\t, selectauth\n",
    "\t, updateauth \n",
    "from syscat.tabauth \n",
    "where grantee='DB2INST1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_perms_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Users Who Have Permissions on a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql obj_perms_by_obj << select substr(grantee,1,8) as grantee\n",
    "\t, controlauth\n",
    "\t, deleteauth\n",
    "\t, insertauth\n",
    "\t, selectauth\n",
    "\t, updateauth \n",
    "from syscat.tabauth \n",
    "where tabschema='DB2INST1' \n",
    "\tand tabname='SALES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_perms_by_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Privileges by User the Easy Way - Querying Object Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql obj_perms << select * \n",
    "from sysibmadm.privileges \n",
    "where authid='DB2INST1'\n",
    "    and objecttype!='DB2 PACKAGE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_perms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Groups a User is a Member Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql groups_for_id << select * from\n",
    "    table(AUTH_LIST_GROUPS_FOR_AUTHID('DB2INST1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(groups_for_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Authorities for an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql auths_for_id << select * from\n",
    "    table(AUTH_LIST_AUTHORITIES_FOR_AUTHID('DB2INST1','U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(auths_for_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complicated SQL for Nicely Formatted Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_auth_ids << with tab_perms as(\n",
    "select rtrim(grantee) as grantee\n",
    "    , granteetype\n",
    "    , count(*) as tab_write_access\n",
    "from syscat.tabauth ta\n",
    "where ta.updateauth='Y' \n",
    "    or ta.insertauth='Y' \n",
    "    or ta.deleteauth='Y'\n",
    "group by grantee, granteetype\n",
    ") ,\n",
    "tab_select as(\n",
    "select rtrim(grantee) as grantee\n",
    "    , granteetype\n",
    "    , count(*) as tab_read_access\n",
    "from syscat.tabauth ta\n",
    "where ta.selectauth='Y' \n",
    "group by grantee, granteetype\n",
    ")\n",
    "select coalesce(da.grantee, tp.grantee) as grantee\n",
    "    , coalesce(da.granteetype, tp.granteetype) as granteetype\n",
    "    , securityadmauth\n",
    "    , dbadmauth\n",
    "    , dataaccessauth\n",
    "    , case dataaccessauth when 'Y' then (select count(*) from syscat.tables where type in ('T','V')) else tp.tab_write_access end as tab_write_access\n",
    "    , case dataaccessauth when 'Y' then (select count(*) from syscat.tables where type in ('T','V')) else ts.tab_read_access end as tab_read_access\n",
    "from syscat.dbauth da\n",
    "    left outer join tab_perms tp on da.grantee=tp.grantee and da.granteetype=tp.granteetype\n",
    "    left outer join tab_select ts on da.grantee=ts.grantee and da.granteetype=ts.granteetype\n",
    "where securityadmauth='Y' or dbadmauth='Y' or dataaccessauth='Y' or tab_write_access > 0 or tab_read_access > 0\n",
    "order by granteetype desc, securityadmauth desc, dbadmauth desc, dataaccessauth desc, tab_write_access desc, tab_read_access desc\n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_auth_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Investigation Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql unused_tables <<     select  t.lastused\n",
    "        , date(stats_time) as stats_time\n",
    "        , date(create_time) as create_time\n",
    "        , t.tabschema,1,10\n",
    "        , t.tabname,1,25\n",
    "        , bigint(card) as table_card\n",
    "        , mt.table_scans\n",
    "        , mt.rows_read\n",
    "        , mt.rows_inserted + mt.rows_updated + mt.rows_deleted as rows_altered\n",
    "        , t.volatile \n",
    "        , mt.member\n",
    "    from    syscat.tables t \n",
    "        join table(mon_get_table('','',-2)) as mt on t.tabschema=mt.tabschema and t.tabname = mt.tabname \n",
    "    where \n",
    "        t.tabschema not like 'SYS%' \n",
    "        and t.tabname not like '%EXPLAIN%' \n",
    "        and t.tabname not like '%ADVISE%' \n",
    "        and t.lastused < current date - 30 days \n",
    "        and type = 'T' \n",
    "        and stats_time is not null and length(t.tabname) < 15 and t.tabschema != 'DBAMON097'\n",
    "    order by t.lastused, t.card desc, t.tabschema, t.tabname \n",
    "    with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unused_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busiest Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql mostused_tables <<  select  t.lastused \n",
    "        , substr(t.tabschema,1,10) as tabschema\n",
    "        , substr(t.tabname,1,25) as tabname\n",
    "        , bigint(card) as table_card \n",
    "        , mt.table_scans\n",
    "        , mt.rows_read\n",
    "        , case when card >0 then mt.rows_read/card else 0 end as avg_reads_per_row\n",
    "        , mt.rows_inserted + mt.rows_updated + mt.rows_deleted as rows_altered\n",
    "        , t.volatile \n",
    "        , mt.member\n",
    "    from    syscat.tables t \n",
    "        join table(mon_get_table('','',-2)) as mt on t.tabschema=mt.tabschema and t.tabname = mt.tabname\n",
    "    where \n",
    "        t.tabschema not like 'SYS%' \n",
    "        and t.tabname not like '%EXPLAIN%' \n",
    "        and t.tabname not like '%ADVISE%' \n",
    "    order by 7 desc, 8 desc, t.tabschema, t.tabname, mt.member \n",
    "    fetch first 20 rows only \n",
    "    with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busy_tab_df=mostused_tables.DataFrame()\n",
    "busy_tab_df['table_card'] = busy_tab_df.apply(lambda x: \"{:,}\".format(x['table_card']), axis=1)\n",
    "busy_tab_df['table_scans'] = busy_tab_df.apply(lambda x: \"{:,}\".format(x['table_scans']), axis=1)\n",
    "busy_tab_df['rows_read'] = busy_tab_df.apply(lambda x: \"{:,}\".format(x['rows_read']), axis=1)\n",
    "busy_tab_df['avg_reads_per_row'] = busy_tab_df.apply(lambda x: \"{:,}\".format(x['avg_reads_per_row']), axis=1)\n",
    "busy_tab_df['rows_altered'] = busy_tab_df.apply(lambda x: \"{:,}\".format(x['rows_altered']), axis=1)\n",
    "display(HTML(busy_tab_df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest Tables by Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql largest_tables_by_size << select  t.lastused, \n",
    "        substr(t.tabschema,1,10) as tabschema, \n",
    "        substr(t.tabname,1,25) as tabname, \n",
    "        bigint(card) as table_card, \n",
    "        data_object_p_size/1024 as data_size_mb, \n",
    "        index_object_p_size/1024 as index_size_mb, \n",
    "        lob_object_p_size/1024 as lob_size_mb, \n",
    "        (ati.data_object_p_size + index_object_p_size + long_object_p_size + lob_object_p_size + xml_object_p_size + col_object_p_size)/1024 as size_mb, \n",
    "        (select listagg(colname ,chr(10)) within group (order by colno) from syscat.columns c where c.tabschema=t.tabschema and c.tabname=t.tabname and typename in ('DATE','TIMESTAMP')) as date_cols, \n",
    "        (select listagg(colname ,chr(10)) within group (order by colno) from syscat.columns c where c.tabschema=t.tabschema and c.tabname=t.tabname and typename like '%LOB') as lob_cols, \n",
    "        t.volatile \n",
    "from    syscat.tables t \n",
    "        join table(mon_get_table('','',-2)) as mt on t.tabschema=mt.tabschema and t.tabname = mt.tabname \n",
    "        join sysibmadm.admintabinfo ati on t.tabschema=ati.tabschema and t.tabname=ati.tabname \n",
    "where \n",
    "        t.tabschema not like 'SYS%' \n",
    "        and t.tabname not like '%EXPLAIN%' \n",
    "        and t.tabname not like '%ADVISE%' \n",
    "order by size_mb desc, t.tabschema, t.tabname \n",
    "fetch first 20 rows only \n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(largest_tables_by_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MON_GET* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Problem SQL in MON_GET_PKG_CACHE_STMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql prob_sql << WITH SUM_TAB (SUM_RR, SUM_CPU, SUM_EXEC, SUM_SORT, SUM_NUM_EXEC) AS ( \n",
    "        SELECT  nullif(FLOAT(SUM(ROWS_READ)),0), \n",
    "                nullif(FLOAT(SUM(TOTAL_CPU_TIME)),0), \n",
    "                nullif(FLOAT(SUM(STMT_EXEC_TIME)),0), \n",
    "                nullif(FLOAT(SUM(TOTAL_SECTION_SORT_TIME)),0), \n",
    "                nullif(FLOAT(SUM(NUM_EXECUTIONS)),0) \n",
    "            FROM TABLE(MON_GET_PKG_CACHE_STMT ( 'D', NULL, NULL, -2)) AS T \n",
    "            WHERE stmt_text not like '%monreport.dbsummary%'\n",
    "        ) \n",
    "SELECT substr(stmt_text,1,25) as STATEMENT, \n",
    "        ROWS_READ, \n",
    "        coalesce(DECIMAL(100*(FLOAT(ROWS_READ)/SUM_TAB.SUM_RR),5,2),0) AS PCT_TOT_RR, \n",
    "        TOTAL_CPU_TIME, \n",
    "        coalesce(DECIMAL(100*(FLOAT(TOTAL_CPU_TIME)/SUM_TAB.SUM_CPU),5,2),0) AS PCT_TOT_CPU, \n",
    "        STMT_EXEC_TIME, \n",
    "        coalesce(DECIMAL(100*(FLOAT(STMT_EXEC_TIME)/SUM_TAB.SUM_EXEC),5,2),0) AS PCT_TOT_EXEC_TIME, \n",
    "        TOTAL_SECTION_SORT_TIME, \n",
    "        coalesce(DECIMAL(100*(FLOAT(TOTAL_SECTION_SORT_TIME)/SUM_TAB.SUM_SORT),5,2),0) AS PCT_TOT_SRT, \n",
    "        NUM_EXECUTIONS, \n",
    "        coalesce(DECIMAL(100*(FLOAT(NUM_EXECUTIONS)/SUM_TAB.SUM_NUM_EXEC),5,2),0) AS PCT_TOT_EXECS, \n",
    "        DECIMAL(FLOAT(STMT_EXEC_TIME)/FLOAT(NUM_EXECUTIONS),10,2) AS AVG_EXEC_TIME, \n",
    "        INSERT_TIMESTAMP,\n",
    "        hex(EXECUTABLE_ID) as EXECUTABLE_ID,\n",
    "        RTRIM(STMT_TEXT) as FULL_STATEMENT \n",
    "    FROM TABLE(MON_GET_PKG_CACHE_STMT ( 'D', NULL, NULL, -2)) AS T, SUM_TAB \n",
    "    WHERE (DECIMAL(100*(FLOAT(ROWS_READ)/SUM_TAB.SUM_RR),5,2) > 10 \n",
    "            OR DECIMAL(100*(FLOAT(TOTAL_CPU_TIME)/SUM_TAB.SUM_CPU),5,2) >10 \n",
    "            OR DECIMAL(100*(FLOAT(STMT_EXEC_TIME)/SUM_TAB.SUM_EXEC),5,2) >10 \n",
    "            OR DECIMAL(100*(FLOAT(TOTAL_SECTION_SORT_TIME)/SUM_TAB.SUM_SORT),5,2) >10 \n",
    "            OR DECIMAL(100*(FLOAT(NUM_EXECUTIONS)/SUM_TAB.SUM_NUM_EXEC),5,2) >10 )\n",
    "        AND stmt_text not like '%monreport.dbsummary%'\n",
    "    ORDER BY ROWS_READ DESC \n",
    "    FETCH FIRST 20 ROWS ONLY \n",
    "    WITH UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=prob_sql.DataFrame()\n",
    "#df = pd.read_csv(r\"C:\\Users\\ecrooks\\Documents\\GitHub\\private_jupyter_notebooks\\problem_sql.csv\")\n",
    "\n",
    "#display(df.columns)\n",
    "df[['pct_tot_rr']]=df[['pct_tot_rr']].astype(float)\n",
    "df[['pct_tot_cpu']]=df[['pct_tot_cpu']].astype(float)\n",
    "df[['pct_tot_exec_time']]=df[['pct_tot_exec_time']].astype(float)\n",
    "df[['pct_tot_srt']]=df[['pct_tot_srt']].astype(float)\n",
    "df[['pct_tot_execs']]=df[['pct_tot_execs']].astype(float)\n",
    "df[['avg_exec_time']]=df[['avg_exec_time']].astype(float)\n",
    "\n",
    "df['rows_read'] = df['rows_read'].map(lambda x: '{:,}'.format(x))\n",
    "df['total_cpu_time'] = df['total_cpu_time'].map(lambda x: '{:,}'.format(x))\n",
    "df['stmt_exec_time'] = df['stmt_exec_time'].map(lambda x: '{:,}'.format(x))\n",
    "df['total_section_sort_time'] = df['total_section_sort_time'].map(lambda x: '{:,}'.format(x))\n",
    "df['num_executions'] = df['num_executions'].map(lambda x: '{:,}'.format(x))\n",
    "\n",
    "#pd.options.display.float_format = '{:,.2f}'.format\n",
    "display(df[['STATEMENT','rows_read','pct_tot_rr','total_cpu_time','pct_tot_cpu','stmt_exec_time','pct_tot_exec_time','total_section_sort_time','pct_tot_srt','num_executions','pct_tot_execs','avg_exec_time']])\n",
    "#df.plot(x='STATEMENT', y=['pct_tot_rr','pct_tot_cpu','pct_tot_exec_time','pct_tot_srt'], kind='barh')\n",
    "#plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=len(df)\n",
    "df_add=df\n",
    "df_add.loc[pos] = pd.Series('OTHER', index = ['STATEMENT'])\n",
    "df_add.at[pos, 'pct_tot_rr'] = 100 - df['pct_tot_rr'].sum()\n",
    "df_add.at[pos, 'pct_tot_cpu'] = 100 - df['pct_tot_cpu'].sum()\n",
    "df_add.at[pos, 'pct_tot_exec_time'] = 100 - df['pct_tot_exec_time'].sum()\n",
    "df_add.at[pos, 'pct_tot_srt'] = 100 - df['pct_tot_srt'].sum()\n",
    "df_add.at[pos, 'pct_tot_execs'] = 100 - df['pct_tot_execs'].sum()\n",
    "df_add['query_num'] = df_add.index\n",
    "df_add['query_num']=df_add['query_num'].apply(lambda x: '{0:0>2}'.format(x))\n",
    "df_add[['query_num']]= 'query' + df[['query_num']]\n",
    "df_add.at[pos, 'query_num'] = 'other'\n",
    "\n",
    "#display(df_add)\n",
    "df_add2=df_add.drop(['rows_read', 'total_cpu_time', 'stmt_exec_time', 'total_section_sort_time', 'num_executions', 'avg_exec_time', 'insert_timestamp', 'executable_id', 'STATEMENT', 'full_statement'], axis=1)\n",
    "#display(df_add2)\n",
    "df_add3=df_add2.set_index('query_num').T\n",
    "\n",
    "df_add3.rename(index={'pct_tot_rr':'Rows Read'},inplace=True)\n",
    "df_add3.rename(index={'pct_tot_cpu':'CPU Time'},inplace=True)\n",
    "df_add3.rename(index={'pct_tot_exec_time':'Execution Time'},inplace=True)\n",
    "df_add3.rename(index={'pct_tot_srt':'Sort Time'},inplace=True)\n",
    "df_add3.rename(index={'pct_tot_execs':'Number of Executions'},inplace=True)\n",
    "#display(df_add3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_add3.plot(kind='barh', title =\"Percent of Resource Consumption by Top Problem Queries\",figsize=(15,10),legend=True, stacked=True, fontsize=12, colormap='Paired')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn=%sql\n",
    "#display(conn)\n",
    "schema='wscomusr'\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "for index, row in df.iterrows():\n",
    "    # skip the \"other\" row added to balance out numbers for the metrics\n",
    "    if row['query_num'] == 'other': \n",
    "        continue\n",
    "    # Display basic information about the query\n",
    "    display(Markdown(\"## Query \"+str(index)))\n",
    "    display(Markdown(\"### Query Characteristics\"))\n",
    "    display(Markdown(\"Executed \"+str(row['num_executions'])+\" times since last placed in the package cache at \"+str(row['insert_timestamp'])))\n",
    "    display(Markdown(\"Consumed \"+str(row['pct_tot_rr'])+\" percent of all rows read by all queries in the package cache.\"))\n",
    "    display(Markdown(\"Consumed \"+str(row['pct_tot_cpu'])+\" percent of all cpu time used by all queries in the package cache.\"))\n",
    "    display(Markdown(\"Consumed \"+str(row['pct_tot_exec_time'])+\" percent of all execution time used by all queries in the package cache.\"))\n",
    "    display(Markdown(\"Consumed \"+str(row['pct_tot_srt'])+\" percent of all sort time used by all queries in the package cache.\"))\n",
    "    display(Markdown(\"### Query Text\"))\n",
    "    formatted_sql=sqlparse.format(df['full_statement'][index], reindent=True)\n",
    "    print(formatted_sql.replace(\"\\\\n\",\"<br>\"))\n",
    "    # If a database connection is available, gather additional information about this query\n",
    "    ## Note: explain may fail if the interval between runing the query to find problem sql and this section was too long, and the section has been cleared from the package cache\n",
    "    if conn:\n",
    "        #When db connection is available\n",
    "        display(Markdown(\"### Query Explain Plan\"))\n",
    "        display(row.dtypes)\n",
    "        exe_id=row['executable_id']\n",
    "        ex_schema=user.upper()\n",
    "        ex_requester=''\n",
    "        ex_time=''\n",
    "        src_name=''\n",
    "        src_schema=schema\n",
    "        src_version=''\n",
    "        %sql call explain_from_section(x'{exe_id}', 'M', NULL, 0, :ex_schema, :ex_requester, :ex_time, :src_name, :src_schema, :src_version)\n",
    "        expln_plan=%sql select * from {user}.last_explained\n",
    "        print(expln_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Log and History Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql backup_list << select date(timestamp(start_time)) as start_date \n",
    "    , time(timestamp(start_time)) as start_time \n",
    "    , start_time as start_timestamp \n",
    "    , dayname(start_time) as day\n",
    "    , timestampdiff ( 4, varchar(timestamp(end_time) - timestamp(start_time)) ) as duration \n",
    "    , case operationtype \n",
    "        when 'D' then 'Delta Offline' \n",
    "        when 'E' then 'Delta Online' \n",
    "        when 'F' then 'Offline' \n",
    "        when 'I' then 'Incremental Offline' \n",
    "        when 'N' then 'Online' \n",
    "        when 'O' then 'Incremental Online' \n",
    "     else operationtype \n",
    "     end || ' ' || case \n",
    "            when objecttype = 'D' then 'DB' \n",
    "            when objecttype = 'P' then 'TS'\n",
    "            else objecttype \n",
    "        end as Type \n",
    "    , devicetype \n",
    "    , sqlcode \n",
    "from sysibmadm.db_history \n",
    "where operation='B' \n",
    "    and start_time > current timestamp - 14 days\n",
    "order by start_date, start_time \n",
    "with ur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(backup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql diag_log << SELECT TIMESTAMP\n",
    "    , substr(APPL_ID,1,15) as APPL_ID_TRUNC\n",
    "    , MSGSEVERITY as SEV\n",
    "    , MSGNUM\n",
    "    , substr(MSG,1,50) as MSG_trunc \n",
    "FROM TABLE ( PD_GET_LOG_MSGS( CURRENT_TIMESTAMP - 7 DAYS)) AS T \n",
    "ORDER BY TIMESTAMP DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(diag_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying System and Database Configuration Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Configuration\n",
    "Note: may not work how you expect it to on docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql system_info << SELECT OS_NAME \n",
    "    , HOST_NAME \n",
    "    , OS_FULL_VERSION \n",
    "    , OS_KERNEL_VERSION \n",
    "    , OS_ARCH_TYPE \n",
    "    , CPU_TOTAL \n",
    "    , CPU_ONLINE \n",
    "    , CPU_CONFIGURED \n",
    "    , CPU_SPEED \n",
    "    , CPU_HMT_DEGREE \n",
    "    , CPU_CORES_PER_SOCKET \n",
    "    , MEMORY_TOTAL \n",
    "    , MEMORY_FREE \n",
    "    , VIRTUAL_MEM_TOTAL\n",
    "    , VIRTUAL_MEM_RESERVED \n",
    "    , VIRTUAL_MEM_FREE \n",
    "    , CPU_LOAD_SHORT \n",
    "    , CPU_LOAD_MEDIUM \n",
    "    , CPU_LOAD_LONG \n",
    "    , CPU_USAGE_TOTAL \n",
    "    , CPU_USER \n",
    "    , CPU_IDLE \n",
    "    , CPU_IOWAIT \n",
    "    , CPU_SYSTEM \n",
    "    , SWAP_PAGE_SIZE \n",
    "    , SWAP_PAGES_IN \n",
    "    , SWAP_PAGES_OUT \n",
    "FROM TABLE(SYSPROC.ENV_GET_SYSTEM_RESOURCES()) AS T \n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(system_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Db2 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql vers_info << SELECT INST_NAME \n",
    "    , IS_INST_PARTITIONABLE \n",
    "    , NUM_DBPARTITIONS \n",
    "    , INST_PTR_SIZE \n",
    "    , RELEASE_NUM \n",
    "    , SERVICE_LEVEL \n",
    "    , BLD_LEVEL \n",
    "    , PTF \n",
    "    , FIXPACK_NUM \n",
    "    , NUM_MEMBERS \n",
    "FROM SYSIBMADM.ENV_INST_INFO \n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vers_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Db2 License Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql lic_info << SELECT INSTALLED_PROD \n",
    "    , INSTALLED_PROD_FULLNAME \n",
    "    , LICENSE_INSTALLED \n",
    "    , PROD_RELEASE \n",
    "    , LICENSE_TYPE \n",
    "from SYSIBMADM.ENV_PROD_INFO \n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic_info_df=lic_info.DataFrame()\n",
    "lic_info_df.style.apply(highlight_equals,threshold='Y',column=['license_installed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Db2 Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql reg_info << SELECT DBPARTITIONNUM\n",
    "    , REG_VAR_NAME \n",
    "    , REG_VAR_VALUE \n",
    "    , IS_AGGREGATE \n",
    "    , AGGREGATE_NAME \n",
    "    , LEVEL \n",
    "from SYSIBMADM.REG_VARIABLES\n",
    "order by DBPARTITIONNUM, REG_VAR_NAME\n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reg_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBM Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql dbm_cfg << SELECT NAME \n",
    "    , VALUE \n",
    "    , VALUE_FLAGS \n",
    "    , DEFERRED_VALUE \n",
    "    , DEFERRED_VALUE_FLAGS \n",
    "from SYSIBMADM.DBMCFG \n",
    "with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbm_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_cfg << SELECT NAME \n",
    "        , VALUE \n",
    "        , VALUE_FLAGS \n",
    "        , DEFERRED_VALUE \n",
    "        , DEFERRED_VALUE_FLAGS \n",
    "    from SYSIBMADM.DBCFG \n",
    "    with ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying DB Storage Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql db_paths << select *\n",
    "from sysibmadm.dbpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
